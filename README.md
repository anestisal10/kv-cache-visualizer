<div align="center">

# ğŸ§  KV-Cache Eviction Visualizer

**An interactive tool for exploring KV-Cache eviction strategies in Large Language Models**

See which tokens survive, which are forgotten, and whether the *attention sink* phenomenon is real.

[![Python 3.10+](https://img.shields.io/badge/Python-3.10+-3776ab?logo=python&logoColor=white)](https://python.org)
[![PyTorch 2.1+](https://img.shields.io/badge/PyTorch-2.1+-ee4c2c?logo=pytorch&logoColor=white)](https://pytorch.org)
[![Gradio](https://img.shields.io/badge/Gradio-6.0+-ff7c00?logo=gradio&logoColor=white)](https://gradio.app)
[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](LICENSE)
[![Tests](https://img.shields.io/badge/Tests-28%2F28_passing-brightgreen)]()

</div>

---

## ğŸ¯ What Is This?

When LLMs generate text, they store **Key-Value (KV) pairs** for every previous token to avoid recomputation. On consumer GPUs (like the RTX 2060 with 6 GB VRAM), this cache quickly fills up. **Eviction policies** decide which tokens to discard â€” and this choice has a dramatic effect on output quality.

This visualizer lets you **watch eviction happen in real time**, compare policies side-by-side, and investigate the [attention sink](https://arxiv.org/abs/2309.17453) hypothesis.

### Research Question

> *Is keeping the first token (the "attention sink") strictly necessary for small models like Qwen-2 0.5B?*

---

## âœ¨ Features

| Feature | Description |
|---------|-------------|
| ğŸ¨ **Token Grid** | Color-coded tokens: ğŸŸ¢ alive, ğŸ”´ evicted, ğŸŸ¡ sink, ğŸ”µ latest |
| ğŸ”¥ **Attention Heatmaps** | Interactive Plotly heatmaps per layer/head |
| ğŸ“Š **Metrics Dashboard** | Perplexity, cache utilization, attention sink tracking |
| âš–ï¸ **Policy Comparison** | Run two policies side-by-side on the same prompt |
| ğŸ”¬ **Automated Experiments** | Test attention sink hypothesis across 6 configurations |
| ğŸš€ **Real-time Streaming** | Watch tokens appear and get evicted step-by-step |

---

## ğŸ“ Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Gradio UI  â”‚â”€â”€â”€â”€â–¶â”‚   Orchestrator   â”‚â”€â”€â”€â”€â–¶â”‚  Model Backend  â”‚
â”‚  (3 tabs)   â”‚     â”‚  (gen loop)      â”‚     â”‚  (HF models)    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                             â”‚
                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                    â”‚  Cache Manager   â”‚
                    â”‚  (evict + track) â”‚
                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                             â”‚
              â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
              â–¼              â–¼              â–¼
        StreamingLLM       H2O        Window-Only
        (sink+window)  (heavy-hitter)  (baseline)
```

---

## âš¡ Eviction Policies

| Policy | Strategy | Key Insight |
|--------|----------|-------------|
| **StreamingLLM** | Keep first N "sink" tokens + sliding window | First token absorbs excess attention via softmax |
| **H2O** | Keep highest cumulative attention + recent window | ~5% of tokens receive >90% of attention mass |
| **Window-Only** | Pure sliding window (no sink) | Does removing the sink actually hurt small models? |
| **Random** | Randomly evict â€” worst case baseline | Lower bound on quality |
| **No Eviction** | Keep everything â€” quality upper bound | Reference for measuring degradation |

---

## ğŸš€ Quick Start

```bash
# Clone the repository
git clone https://github.com/YOUR_USERNAME/KV-Cache-Visualizer.git
cd KV-Cache-Visualizer

# Install dependencies
pip install -r requirements.txt

# Launch the visualizer
python app.py
```

Then open **http://localhost:7860** in your browser.

> **First run** will download Qwen2-0.5B-Instruct (~1 GB). Subsequent runs use the cached model.

---

## ğŸ”¬ Running Experiments

Test whether the attention sink is necessary:

```bash
python -m src.experiments.attention_sink_experiment
```

This runs **6 configurations** (no eviction, StreamingLLM with 4/1/0 sinks, window-only, random) across multiple prompts and saves results to `results/`.

---

## ğŸ§ª Running Tests

```bash
# Fast tests (no GPU or model download required)
pytest tests/ -v -m "not slow"

# Full suite including model integration tests
pytest tests/ -v -m slow
```

**28 tests** covering all eviction policies and the cache manager.

---

## ğŸ› ï¸ Target Hardware

Designed for **consumer GPUs**. Tested on **NVIDIA RTX 2060 (6 GB VRAM)**.

| Model | VRAM (FP16) | Quantization |
|-------|------------|--------------|
| Qwen2-0.5B-Instruct | ~1 GB | None needed |
| Qwen2-1.5B-Instruct | ~3 GB | None needed |
| Phi-3-mini-4k-instruct | ~4 GB | 4-bit auto-applied |

---

## ğŸ“ Project Structure

```
â”œâ”€â”€ app.py                              # Entry point â€” launches Gradio
â”œâ”€â”€ requirements.txt                    # Python dependencies
â”œâ”€â”€ configs/
â”‚   â””â”€â”€ default.yaml                    # Default configuration
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ model_backend.py                # HuggingFace model loading & attention extraction
â”‚   â”œâ”€â”€ cache_manager.py                # KV-Cache wrapper with eviction & history tracking
â”‚   â”œâ”€â”€ orchestrator.py                 # Token-by-token generation loop
â”‚   â”œâ”€â”€ eviction_policies/
â”‚   â”‚   â”œâ”€â”€ base.py                     # Abstract base class
â”‚   â”‚   â”œâ”€â”€ streaming_llm.py            # Sink + sliding window
â”‚   â”‚   â”œâ”€â”€ h2o.py                      # Heavy-Hitter Oracle
â”‚   â”‚   â”œâ”€â”€ window_only.py              # Pure sliding window
â”‚   â”‚   â”œâ”€â”€ random_evict.py             # Random baseline
â”‚   â”‚   â””â”€â”€ no_eviction.py              # Full cache upper bound
â”‚   â”œâ”€â”€ ui/
â”‚   â”‚   â”œâ”€â”€ app.py                      # Main Gradio layout (3 tabs)
â”‚   â”‚   â”œâ”€â”€ token_grid.py               # Colored HTML token visualization
â”‚   â”‚   â”œâ”€â”€ heatmap.py                  # Plotly attention heatmaps
â”‚   â”‚   â””â”€â”€ metrics.py                  # Perplexity & stats charts
â”‚   â””â”€â”€ experiments/
â”‚       â”œâ”€â”€ attention_sink_experiment.py # Automated experiment runner
â”‚       â””â”€â”€ utils.py                    # Result saving utilities
â”œâ”€â”€ tests/                              # Unit tests (28 tests)
â”œâ”€â”€ collective_intelligence.md          # Detailed design document
â””â”€â”€ LICENSE
```

---

## ğŸ“š References

- **StreamingLLM** â€” Xiao et al., *"Efficient Streaming Language Models with Attention Sinks"*, ICLR 2024 â€” [arXiv:2309.17453](https://arxiv.org/abs/2309.17453)
- **H2O** â€” Zhang et al., *"Hâ‚‚O: Heavy-Hitter Oracle for Efficient Generative Inference of Large Language Models"*, NeurIPS 2023
- **Attention Sinks** â€” The observation that the first token absorbs disproportionate attention mass due to softmax normalization

---

## ğŸ“„ License

This project is licensed under the MIT License â€” see [LICENSE](LICENSE) for details.
